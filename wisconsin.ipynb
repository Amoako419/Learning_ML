{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 569\n",
      "Number of target instances: 569\n",
      "Number of features: 30\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Print the number of rows (instances), columns (features) in the dataset, and the number of target instances\n",
    "print(\"Number of instances:\", len(data.data))\n",
    "# TODO: print the number of features\n",
    "print(\"Number of target instances:\", len(data.target))\n",
    "print(\"Number of features:\", len(data.feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 212, 1: 357}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Intentional Bug: 'np.unique' should be used to find unique elements and their counts\n",
    "# The current code tries to directly create a dictionary, which will not provide the correct counts\n",
    "unique,counts = np.unique(data.target,return_counts = True)\n",
    "print(dict(zip(unique,counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       mean radius  mean texture  mean perimeter    mean area  \\\n",
      "count   569.000000    569.000000      569.000000   569.000000   \n",
      "mean     14.127292     19.289649       91.969033   654.889104   \n",
      "std       3.524049      4.301036       24.298981   351.914129   \n",
      "min       6.981000      9.710000       43.790000   143.500000   \n",
      "25%      11.700000     16.170000       75.170000   420.300000   \n",
      "50%      13.370000     18.840000       86.240000   551.100000   \n",
      "75%      15.780000     21.800000      104.100000   782.700000   \n",
      "max      28.110000     39.280000      188.500000  2501.000000   \n",
      "\n",
      "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
      "count       569.000000        569.000000      569.000000           569.000000   \n",
      "mean          0.096360          0.104341        0.088799             0.048919   \n",
      "std           0.014064          0.052813        0.079720             0.038803   \n",
      "min           0.052630          0.019380        0.000000             0.000000   \n",
      "25%           0.086370          0.064920        0.029560             0.020310   \n",
      "50%           0.095870          0.092630        0.061540             0.033500   \n",
      "75%           0.105300          0.130400        0.130700             0.074000   \n",
      "max           0.163400          0.345400        0.426800             0.201200   \n",
      "\n",
      "       mean symmetry  mean fractal dimension  ...  worst radius  \\\n",
      "count     569.000000              569.000000  ...    569.000000   \n",
      "mean        0.181162                0.062798  ...     16.269190   \n",
      "std         0.027414                0.007060  ...      4.833242   \n",
      "min         0.106000                0.049960  ...      7.930000   \n",
      "25%         0.161900                0.057700  ...     13.010000   \n",
      "50%         0.179200                0.061540  ...     14.970000   \n",
      "75%         0.195700                0.066120  ...     18.790000   \n",
      "max         0.304000                0.097440  ...     36.040000   \n",
      "\n",
      "       worst texture  worst perimeter   worst area  worst smoothness  \\\n",
      "count     569.000000       569.000000   569.000000        569.000000   \n",
      "mean       25.677223       107.261213   880.583128          0.132369   \n",
      "std         6.146258        33.602542   569.356993          0.022832   \n",
      "min        12.020000        50.410000   185.200000          0.071170   \n",
      "25%        21.080000        84.110000   515.300000          0.116600   \n",
      "50%        25.410000        97.660000   686.500000          0.131300   \n",
      "75%        29.720000       125.400000  1084.000000          0.146000   \n",
      "max        49.540000       251.200000  4254.000000          0.222600   \n",
      "\n",
      "       worst compactness  worst concavity  worst concave points  \\\n",
      "count         569.000000       569.000000            569.000000   \n",
      "mean            0.254265         0.272188              0.114606   \n",
      "std             0.157336         0.208624              0.065732   \n",
      "min             0.027290         0.000000              0.000000   \n",
      "25%             0.147200         0.114500              0.064930   \n",
      "50%             0.211900         0.226700              0.099930   \n",
      "75%             0.339100         0.382900              0.161400   \n",
      "max             1.058000         1.252000              0.291000   \n",
      "\n",
      "       worst symmetry  worst fractal dimension  \n",
      "count      569.000000               569.000000  \n",
      "mean         0.290076                 0.083946  \n",
      "std          0.061867                 0.018061  \n",
      "min          0.156500                 0.055040  \n",
      "25%          0.250400                 0.071460  \n",
      "50%          0.282200                 0.080040  \n",
      "75%          0.317900                 0.092080  \n",
      "max          0.663800                 0.207500  \n",
      "\n",
      "[8 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary module\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Extract features and target variable\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Convert features to DataFrame\n",
    "data_df = pd.DataFrame(X, columns=data.feature_names)\n",
    "\n",
    "# Display the summary statistics for the DataFrame\n",
    "print(data_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the data for optimal performance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# TODO: Define the parameter grid to search for the optimal 'C' value\n",
    "param_grid = {'C' : [0.001,0.01,0.1,1,10,100]}\n",
    "\n",
    "# TODO: Instantiate GridSearchCV with LogisticRegression and the defined parameter grid\n",
    "grid_search = GridSearchCV(LogisticRegression(max_iter = 10000),param_grid,cv=5)\n",
    "\n",
    "# Fit the grid with data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# TODO: Load the breast cancer dataset and split it into training and testing sets\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 43)\n",
    "\n",
    "# TODO: Scale the training and testing data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# TODO: Define a parameter grid with 'C' values ranging from very small to large and both L1 and L2 penalties\n",
    "param_grid = {'C':[0.001,0.01,0.1,1,10,100]}\n",
    "# TODO: Instantiate GridSearchCV with a LogisticRegression solver set to the parameter grid, and a 5-fold cross-validation\n",
    "grid_search = GridSearchCV(LogisticRegression(max_iter = 10000),param_grid,cv=5)\n",
    "\n",
    "# TODO: Fit the grid search object to the scaled training data and print out the best parameters found\n",
    "grid_search.fit(X_train_scaled,y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 3, 'min_samples_split': 3}\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Parameter values under consideration\n",
    "param_grid = {'max_depth': range(3, 10), 'min_samples_split': range(2, 4)}\n",
    "\n",
    "# Creating the GridSearchCV object\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
    "\n",
    "# Fitting the model to our training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Fetching and printing the best found parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of L1 Regularized Model: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Breast Cancer Wisconsin Dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Splitting the dataset into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Applying L1 regularization on Logistic Regression and training the model\n",
    "logistic_l1 = LogisticRegression(penalty='l2', C=0.1, solver='liblinear', max_iter=10000)\n",
    "logistic_l1.fit(X_train, Y_train)\n",
    "Y_pred = logistic_l1.predict(X_test)\n",
    "print(f\"Accuracy of L1 Regularized Model: {accuracy_score(Y_test, Y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of L2 Regularized Model: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Breast Cancer Wisconsin Dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Splitting the dataset into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# TODO: Apply L2 regularization on Logistic Regression and train the model\n",
    "logistic_l2 = LogisticRegression(penalty='l2',C=0.1,solver='liblinear',max_iter=10000)\n",
    "# Hint: Use LogisticRegression with appropriate parameters for L2 regularization\n",
    "logistic_l2.fit(X_train,Y_train)\n",
    "# Predicting the test set results\n",
    "Y_pred = logistic_l2.predict(X_test)\n",
    "# accuracy_score = accuracy_score(Y_test,Y_pred)\n",
    "# TODO: Calculate and print the accuracy of the model\n",
    "print(f\"Accuracy of L2 Regularized Model: {accuracy_score(Y_test, Y_pred):.2f}\")\n",
    "# Hint: Use the accuracy_score function and print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Accuracy:  0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=150, max_features='sqrt') # Your code here (modify the number of estimators)\n",
    "rfc.fit(X_train, y_train) # Fit the classifier on the training set\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Print accuracy score\n",
    "print('RandomForest Accuracy: ', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoosting Accuracy: 0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# TODO: Load the dataset\n",
    "data = load_breast_cancer()\n",
    "# TODO: Assign X and y\n",
    "X,y = data.data,data.target\n",
    "\n",
    "# TODO: Split the dataset into training and testing sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=43)\n",
    "\n",
    "# Train a GradientBoostingClassifier with adjusted number of estimators\n",
    "gbc = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=3) # Adjusted number of sequential trees\n",
    "gbc.fit(X_train, y_train) # fit the classifier on the training set\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = gbc.predict(X_test)\n",
    "\n",
    "# Print accuracy score\n",
    "print('GradientBoosting Accuracy:', gbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy : 0.9590643274853801\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: Load the Wisconsin breast cancer dataset into X, y\n",
    "data = load_breast_cancer()\n",
    "X,y = data.data,data.target\n",
    "\n",
    "# TODO: Split the dataset into a training set and a testing set\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state=3)\n",
    "\n",
    "# TODO: Instantiate a GradientBoostingClassifier with an adjusted learning rate\n",
    "gbc = GradientBoostingClassifier(n_estimators=100,learning_rate=0.1,max_depth=3)\n",
    "\n",
    "# TODO: Fit the classifier on the training data\n",
    "gbc.fit(X_train,y_train)\n",
    "\n",
    "# TODO: Predict the labels for the test set and print the accuracy score\n",
    "y_pred = gbc.predict(X_test)\n",
    "accurancy = gbc.score(X_test,y_test)\n",
    "print('The accuracy :',accurancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy:  0.9790209790209791\n",
      "Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        54\n",
      "           1       0.99      0.98      0.98        89\n",
      "\n",
      "    accuracy                           0.98       143\n",
      "   macro avg       0.98      0.98      0.98       143\n",
      "weighted avg       0.98      0.98      0.98       143\n",
      "\n",
      "Random Forest Accuracy: 0.965034965034965\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95        54\n",
      "           1       0.97      0.98      0.97        89\n",
      "\n",
      "    accuracy                           0.97       143\n",
      "   macro avg       0.96      0.96      0.96       143\n",
      "weighted avg       0.97      0.97      0.96       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the data\n",
    "data = load_breast_cancer()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# TODO: Scale the training and testing features using the right scaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply Logistic Regression with the scaled data\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "# TODO: Fit the Logistic Regression model on the scaled training data\n",
    "lr.fit(X_train_scaled,y_train)\n",
    "# Make predictions\n",
    "# TODO: Replace this with actual scaled test data variable\n",
    "lr_pred = lr.predict(X_test_scaled)\n",
    "\n",
    "# Print accuracy score and classification report for logistic regression\n",
    "print(\"Logistic Regression Accuracy: \", accuracy_score(y_test, lr_pred))\n",
    "print(\"Logistic Regression Classification Report:\\n\", classification_report(y_test, lr_pred))\n",
    "\n",
    "# TODO: Apply Random Forest with the scaled data\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_scaled,y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_pred = rf.predict(X_test_scaled)\n",
    "\n",
    "# Print accuracy score and classification report for random forest\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_pred))\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performing model: lr with accuracy: 0.4348\n",
      "Final Model Accuracy: 0.43478260869565216\n",
      "Classification Report of the Best Model:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      1.00      0.61        20\n",
      "           1       0.00      0.00      0.00        26\n",
      "\n",
      "    accuracy                           0.43        46\n",
      "   macro avg       0.22      0.50      0.30        46\n",
      "weighted avg       0.19      0.43      0.26        46\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Load and split the dataset into training and testing sets\n",
    "data = load_breast_cancer()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.08, random_state=1)\n",
    "\n",
    "# Scale the training and testing features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Set up logistic regression with hyperparameter tuning\n",
    "lr_params = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l2']}\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf_lr = GridSearchCV(lr, lr_params, cv=5)\n",
    "clf_lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Set up and fit Random Forest and Gradient Boosting Classifiers\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=3)\n",
    "gbc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate each model and store accuracy in a dictionary\n",
    "models = {'lr': clf_lr.score(X_test, y_test), 'rf': rf.score(X_test, y_test), 'gbc': gbc.score(X_test, y_test)}\n",
    "\n",
    "# Find the best performing model\n",
    "best_model_name = max(models, key=models.get)\n",
    "best_model_score = models[best_model_name]\n",
    "\n",
    "print(f\"Best performing model: {best_model_name} with accuracy: {best_model_score:.4f}\")\n",
    "\n",
    "# Evaluate the best model using accuracy and classification report\n",
    "print(\"Final Model Accuracy:\", \n",
    "      models[best_model_name] if best_model_name != 'rf' else rf.score(X_test, y_test))\n",
    "\n",
    "print(\"Classification Report of the Best Model:\\n\", \n",
    "      classification_report(y_test, \n",
    "          clf_lr.predict(X_test) if best_model_name == 'lr' else (rf.predict(X_test) if best_model_name == 'rf' else gbc.predict(X_test))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;linearsvc&#x27;, LinearSVC(C=1, random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;linearsvc&#x27;, LinearSVC(C=1, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=1, random_state=42)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('linearsvc', LinearSVC(C=1, random_state=42))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = (iris.target == 2) # Iris virginica\n",
    "svm_clf = make_pipeline(StandardScaler(),LinearSVC(C=1, random_state=42))\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = [[5.5, 1.7], [5.0, 1.5]]\n",
    "svm_clf.predict(X_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
